# Evaluation Metrics – Week 4 🧮

This notebook is part of my work for **Week 4** of the Machine Learning Zoomcamp by [DataTalks.Club](https://datatalks.club/).  
In this stage, I focused on evaluating classification models built in earlier weeks by applying key metrics and interpreting model performance.

## Key Highlights
- 📋 Computation of confusion matrix, accuracy, precision, recall, and F1-score  
- 📈 Calculation and interpretation of ROC-AUC for binary classification  
- 🎯 Discussion of metric trade-offs (e.g., when accuracy isn’t enough)  
- 🔍 Visualization of metric behavior for the lead scoring problem

## Files
- `Evaluation_Metrics.ipynb` — Jupyter notebook with evaluation steps, code, and plots

## Results
This notebook demonstrates how evaluation metrics provide deeper insight into model performance — beyond simple accuracy — helping determine whether a model is suitable for deployment.

📘 **Notebook link:** (https://github.com/BrutBoss/Machine_Learning_Zoomcamp_Homework/blob/main/04-Evaluation/Evaluation_Metrics.ipynb)
