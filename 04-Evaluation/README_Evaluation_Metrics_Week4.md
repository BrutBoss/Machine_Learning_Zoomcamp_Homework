# Evaluation Metrics â€“ Week 4 ğŸ§®

This notebook is part of my work for **Week 4** of the Machine Learning Zoomcamp by [DataTalks.Club](https://datatalks.club/).  
In this stage, I focused on evaluating classification models built in earlier weeks by applying key metrics and interpreting model performance.

## Key Highlights
- ğŸ“‹ Computation of confusion matrix, accuracy, precision, recall, and F1-score  
- ğŸ“ˆ Calculation and interpretation of ROC-AUC for binary classification  
- ğŸ¯ Discussion of metric trade-offs (e.g., when accuracy isnâ€™t enough)  
- ğŸ” Visualization of metric behavior for the lead scoring problem

## Files
- `Evaluation_Metrics.ipynb` â€” Jupyter notebook with evaluation steps, code, and plots

## Results
This notebook demonstrates how evaluation metrics provide deeper insight into model performance â€” beyond simple accuracy â€” helping determine whether a model is suitable for deployment.

ğŸ“˜ **Notebook link:** (https://github.com/BrutBoss/Machine_Learning_Zoomcamp_Homework/blob/main/04-Evaluation/Evaluation_Metrics.ipynb)
